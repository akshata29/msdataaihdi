{"nbformat_minor": 2, "cells": [{"source": "### Spark streaming has microbatching, which means data comes as batches and executers run on the batches of data. If the executor has idle timeout less than the time it takes to process the batch, then the executors would be constantly added and removed. If the executors idle timeout is greater than the batch duration, the executor never gets removed. So we recommend that you disable dynamic allocation by setting spark.dynamicAllocation.enabled to false when running streaming applications.", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 1, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0\",\n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\",\n        \"spark.dynamicAllocation.enabled\": false\n    }\n}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'conf': {u'spark.jars.packages': u'org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0', u'spark.dynamicAllocation.enabled': False, u'spark.jars.excludes': u'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1595680205259_0008</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn0-sparkh.uwguulvqr1kevfnztmsfrc5rqc.bx.internal.cloudapp.net:8088/proxy/application_1595680205259_0008/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn4-sparkh.uwguulvqr1kevfnztmsfrc5rqc.bx.internal.cloudapp.net:30060/node/containerlogs/container_e01_1595680205259_0008_01_000001/livy\">Link</a></td><td></td></tr></table>"}, "metadata": {}}], "metadata": {"cell_status": {"execute_time": {"duration": 55.885986328125, "end_time": 1595701605662.698}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "### Create the Kafka topic. Edit the command below by replacing YOUR_ZOOKEEPER_HOSTS with the Zookeeper host information extracted in the first step. Enter the edited command in your Jupyter Notebook to create the tripdata topic.", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 2, "cell_type": "code", "source": "%%bash\nexport KafkaZookeepers=\"zk0-kafkah.uwguulvqr1kevfnztmsfrc5rqc.bx.internal.cloudapp.net:2181,zk1-kafkah.uwguulvqr1kevfnztmsfrc5rqc.bx.internal.cloudapp.net:2181,zk6-kafkah.uwguulvqr1kevfnztmsfrc5rqc.bx.internal.cloudapp.net:2181\"\n\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic tripdata --zookeeper $KafkaZookeepers", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Created topic \"tripdata\".\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 8595.529052734375, "end_time": 1595701658247.53}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "### Retrieve data on taxi trips. Enter the command in the next cell to load data on taxi trips in New York City. The data is loaded into a dataframe and then the dataframe is displayed as the cell output.", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 3, "cell_type": "code", "source": "import spark.implicits._\n\n// Load the data from the New York City Taxi data REST API for 2016 Green Taxi Trip Data\nval url=\"https://data.cityofnewyork.us/resource/pqfs-mqru.json\"\nval result = scala.io.Source.fromURL(url).mkString\n\n// Create a dataframe from the JSON data\nval taxiDF = spark.read.json(Seq(result).toDS)\n\n// Display the dataframe containing trip data\ntaxiDF.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1595680205259_0009</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn0-sparkh.uwguulvqr1kevfnztmsfrc5rqc.bx.internal.cloudapp.net:8088/proxy/application_1595680205259_0009/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn4-sparkh.uwguulvqr1kevfnztmsfrc5rqc.bx.internal.cloudapp.net:30060/node/containerlogs/container_e01_1595680205259_0009_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n+------------------+-------------------+-----+-----------+---------------------+---------------------+--------------------+-------+---------------+------------+------------------+-------------------+----------+------------------+----------+------------+------------+-------------+---------+--------+\n|  dropoff_latitude|  dropoff_longitude|extra|fare_amount|improvement_surcharge|lpep_dropoff_datetime|lpep_pickup_datetime|mta_tax|passenger_count|payment_type|   pickup_latitude|   pickup_longitude|ratecodeid|store_and_fwd_flag|tip_amount|tolls_amount|total_amount|trip_distance|trip_type|vendorid|\n+------------------+-------------------+-----+-----------+---------------------+---------------------+--------------------+-------+---------------+------------+------------------+-------------------+----------+------------------+----------+------------+------------+-------------+---------+--------+\n|40.698043823242188|-73.924278259277344|  0.5|          8|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:29:...|    0.5|              1|           1|40.680610656738281|-73.928642272949219|         1|                 N|      1.86|           0|       11.16|         1.46|        1|       2|\n|40.761379241943359|-73.923919677734375|  0.5|       15.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:19:...|    0.5|              1|           2|40.723175048828125|-73.952674865722656|         1|                 N|         0|           0|        16.8|         3.56|        1|       2|\n|40.646072387695313|-74.013160705566406|  0.5|       16.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:19:...|    0.5|              1|           1|40.676105499267578|-73.971611022949219|         1|                 N|      4.45|           0|       22.25|         3.79|        1|       2|\n|40.689033508300781|-74.000648498535156|  0.5|       13.5|                  0.3| 2016-01-01T00:38:...|2016-01-01T00:22:...|    0.5|              1|           2|40.669578552246094|   -73.989501953125|         1|                 N|         0|           0|        14.8|         3.01|        1|       2|\n|40.663013458251953|-73.940719604492188|  0.5|         12|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:24:...|    0.5|              1|           2|40.682853698730469|-73.964729309082031|         1|                 N|         0|           0|        13.3|         2.55|        1|       2|\n|40.742111206054688|-73.867744445800781|  0.5|          7|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:32:...|    0.5|              1|           2|40.746456146240234|-73.891143798828125|         1|                 N|         0|           0|         8.3|         1.37|        1|       2|\n|40.745689392089844|-73.886192321777344|  0.5|          5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:34:...|    0.5|              1|           2|40.746196746826172|-73.896675109863281|         1|                 N|         0|           0|         6.3|         0.57|        1|       2|\n|40.794120788574219|-73.949150085449219|  0.5|          7|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:31:...|    0.5|              1|           2|40.803558349609375|-73.953353881835937|         1|                 N|         0|           0|         8.3|         1.01|        1|       2|\n|40.679725646972656|-73.971572875976562|  0.5|         12|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:24:...|    0.5|              1|           1|40.702816009521484|-73.994064331054688|         1|                 N|         2|           0|        15.3|         2.46|        1|       2|\n|40.739658355712891|-73.917549133300781|  0.5|          9|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:28:...|    0.5|              1|           1|40.756641387939453|-73.914131164550781|         1|                 N|       1.6|           0|        11.9|         1.61|        1|       2|\n|40.763126373291016|-73.921028137207031|  0.5|          6|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:32:...|    0.5|              1|           1|40.761829376220703|-73.911178588867188|         1|                 N|      1.82|           0|        9.12|         0.72|        1|       2|\n|40.718177795410156|-73.962753295898438|  0.5|        3.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:37:...|    0.5|              1|           1|40.715328216552734|-73.958168029785156|         1|                 N|      0.96|           0|        5.76|         0.32|        1|       2|\n|40.842765808105469|-73.924903869628906|  0.5|       14.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:21:...|    0.5|              1|           2|40.800785064697266|-73.946678161621094|         1|                 N|         0|           0|        15.8|         3.54|        1|       2|\n|40.775833129882812| -73.90240478515625|  0.5|          6|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:34:...|    0.5|              1|           2|40.763439178466797|-73.914291381835938|         1|                 N|         0|           0|         7.3|         1.10|        1|       2|\n|40.850196838378906|-73.932029724121094|  0.5|         11|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:26:...|    0.5|              1|           1|40.824314117431641|-73.943374633789063|         1|                 N|         2|           0|        14.3|         2.28|        1|       2|\n|40.640159606933594|-73.966880798339844|  0.5|        4.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:35:...|    0.5|              1|           1|40.632152557373047| -73.96697998046875|         1|                 N|      1.16|           0|        6.96|         0.68|        1|       2|\n|40.811866760253906|-73.951583862304688|  0.5|         10|                  0.3| 2016-01-01T00:38:...|2016-01-01T00:25:...|    0.5|              1|           2|40.814445495605469|-73.937843322753906|         1|                 N|         0|           0|        11.3|         1.36|        1|       2|\n|40.693325042724609|-73.947746276855469|  0.5|       15.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:17:...|    0.5|              1|           1|40.690513610839844|-73.990364074707031|         1|                 N|         3|           0|        19.8|         3.07|        1|       2|\n|40.751564025878906|-73.855522155761719|  0.5|        7.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:31:...|    0.5|              1|           2|    40.74755859375|-73.883827209472656|         1|                 N|         0|           0|         8.8|         1.52|        1|       2|\n|40.659637451171875|-73.976943969726563|  0.5|       11.5|                  0.3| 2016-01-01T00:39:...|2016-01-01T00:25:...|    0.5|              1|           2|40.684413909912109|-73.980354309082031|         1|                 N|         0|           0|        12.8|         2.55|        1|       2|\n+------------------+-------------------+-----+-----------+---------------------+---------------------+--------------------+-------+---------------+------------+------------------+-------------------+----------+------------------+----------+------------+------------+-------------+---------+--------+\nonly showing top 20 rows"}], "metadata": {"cell_status": {"execute_time": {"duration": 9330.049072265625, "end_time": 1595701742515.474}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "### Set the Kafka broker hosts information. Replace YOUR_KAFKA_BROKER_HOSTS with the broker hosts information you extracted in step 1. Enter the edited command in the next Jupyter Notebook cell.", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 4, "cell_type": "code", "source": "// The Kafka broker hosts and topic used to write to Kafka\nval kafkaBrokers=\"wn0-kafkah.uwguulvqr1kevfnztmsfrc5rqc.bx.internal.cloudapp.net:9092,wn1-kafkah.uwguulvqr1kevfnztmsfrc5rqc.bx.internal.cloudapp.net:9092,wn2-kafkah.uwguulvqr1kevfnztmsfrc5rqc.bx.internal.cloudapp.net:9092\"\nval kafkaTopic=\"tripdata\"\n\nprintln(\"Finished setting Kafka broker and topic configuration.\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Finished setting Kafka broker and topic configuration."}], "metadata": {"cell_status": {"execute_time": {"duration": 769.769775390625, "end_time": 1595701824784.373}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "### Send the data to Kafka. In the following command, the vendorid field is used as the key value for the Kafka message. The key is used by Kafka when partitioning data. All of the fields are stored in the Kafka message as a JSON string value. Enter the following command in Jupyter to save the data to Kafka using a batch query.", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 5, "cell_type": "code", "source": "// Select the vendorid as the key and save the JSON string as the value.\nval query = taxiDF.selectExpr(\"CAST(vendorid AS STRING) as key\", \"to_JSON(struct(*)) AS value\").write.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafkaBrokers).option(\"topic\", kafkaTopic).save()\n\nprintln(\"Data sent to Kafka\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Data sent to Kafka"}], "metadata": {"cell_status": {"execute_time": {"duration": 3269.35498046875, "end_time": 1595701855940.738}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "### Declare a schema. The following command demonstrates how to use a schema when reading JSON data from kafka. Enter the command in your next Jupyter cell.", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 6, "cell_type": "code", "source": "// Import bits useed for declaring schemas and working with JSON data\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\n\n// Define a schema for the data\nval schema = (new StructType).add(\"dropoff_latitude\", StringType).add(\"dropoff_longitude\", StringType).add(\"extra\", StringType).add(\"fare_amount\", StringType).add(\"improvement_surcharge\", StringType).add(\"lpep_dropoff_datetime\", StringType).add(\"lpep_pickup_datetime\", StringType).add(\"mta_tax\", StringType).add(\"passenger_count\", StringType).add(\"payment_type\", StringType).add(\"pickup_latitude\", StringType).add(\"pickup_longitude\", StringType).add(\"ratecodeid\", StringType).add(\"store_and_fwd_flag\", StringType).add(\"tip_amount\", StringType).add(\"tolls_amount\", StringType).add(\"total_amount\", StringType).add(\"trip_distance\", StringType).add(\"trip_type\", StringType).add(\"vendorid\", StringType)\n// Reproduced here for readability\n//val schema = (new StructType)\n//   .add(\"dropoff_latitude\", StringType)\n//   .add(\"dropoff_longitude\", StringType)\n//   .add(\"extra\", StringType)\n//   .add(\"fare_amount\", StringType)\n//   .add(\"improvement_surcharge\", StringType)\n//   .add(\"lpep_dropoff_datetime\", StringType)\n//   .add(\"lpep_pickup_datetime\", StringType)\n//   .add(\"mta_tax\", StringType)\n//   .add(\"passenger_count\", StringType)\n//   .add(\"payment_type\", StringType)\n//   .add(\"pickup_latitude\", StringType)\n//   .add(\"pickup_longitude\", StringType)\n//   .add(\"ratecodeid\", StringType)\n//   .add(\"store_and_fwd_flag\", StringType)\n//   .add(\"tip_amount\", StringType)\n//   .add(\"tolls_amount\", StringType)\n//   .add(\"total_amount\", StringType)\n//   .add(\"trip_distance\", StringType)\n//   .add(\"trip_type\", StringType)\n//   .add(\"vendorid\", StringType)\n\nprintln(\"Schema declared\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Schema declared"}], "metadata": {"cell_status": {"execute_time": {"duration": 1331.010009765625, "end_time": 1595701884780.5}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "### Select data and start the stream. The following command demonstrates how to retrieve data from Kafka using a batch query. And then write the results out to HDFS on the Spark cluster. In this example, the select retrieves the message (value field) from Kafka and applies the schema to it. The data is then written to HDFS (WASB or ADL) in parquet format. Enter the command in your next Jupyter cell.", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 11, "cell_type": "code", "source": "// Read a batch from Kafka\nval kafkaDF = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafkaBrokers).option(\"subscribe\", kafkaTopic).option(\"startingOffsets\", \"earliest\").load()\n\n// Select data and write to file\nval query = kafkaDF.select(from_json(col(\"value\").cast(\"string\"), schema) as \"trip\").write.format(\"parquet\").option(\"path\",\"/nyctaxiraw/batchtripdata\").option(\"checkpointLocation\", \"/batchcheckpoint\").save()\n\nprintln(\"Wrote data to file\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Wrote data to file"}], "metadata": {"cell_status": {"execute_time": {"duration": 7315.661865234375, "end_time": 1595702105951.622}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "### You can verify that the files were created by entering the command in your next Jupyter cell. It lists the files in the /example/batchtripdata directory.", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 12, "cell_type": "code", "source": "%%bash\nhdfs dfs -ls /nyctaxiraw/batchtripdata", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Found 9 items\n-rw-r--r--   1 livy supergroup          0 2020-07-25 18:35 /nyctaxiraw/batchtripdata/_SUCCESS\n-rw-r--r--   1 livy supergroup       2160 2020-07-25 18:35 /nyctaxiraw/batchtripdata/part-00000-e15b37f9-7e6c-4db7-a4c3-1b3902cb720f-c000.snappy.parquet\n-rw-r--r--   1 livy supergroup       2160 2020-07-25 18:35 /nyctaxiraw/batchtripdata/part-00001-e15b37f9-7e6c-4db7-a4c3-1b3902cb720f-c000.snappy.parquet\n-rw-r--r--   1 livy supergroup       2160 2020-07-25 18:35 /nyctaxiraw/batchtripdata/part-00002-e15b37f9-7e6c-4db7-a4c3-1b3902cb720f-c000.snappy.parquet\n-rw-r--r--   1 livy supergroup       2160 2020-07-25 18:35 /nyctaxiraw/batchtripdata/part-00003-e15b37f9-7e6c-4db7-a4c3-1b3902cb720f-c000.snappy.parquet\n-rw-r--r--   1 livy supergroup       2160 2020-07-25 18:35 /nyctaxiraw/batchtripdata/part-00004-e15b37f9-7e6c-4db7-a4c3-1b3902cb720f-c000.snappy.parquet\n-rw-r--r--   1 livy supergroup       2160 2020-07-25 18:35 /nyctaxiraw/batchtripdata/part-00005-e15b37f9-7e6c-4db7-a4c3-1b3902cb720f-c000.snappy.parquet\n-rw-r--r--   1 livy supergroup      69270 2020-07-25 18:35 /nyctaxiraw/batchtripdata/part-00006-e15b37f9-7e6c-4db7-a4c3-1b3902cb720f-c000.snappy.parquet\n-rw-r--r--   1 livy supergroup       2160 2020-07-25 18:35 /nyctaxiraw/batchtripdata/part-00007-e15b37f9-7e6c-4db7-a4c3-1b3902cb720f-c000.snappy.parquet\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 3012.284912109375, "end_time": 1595702111071.832}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "### While the previous example used a batch query, the following command demonstrates how to do the same thing using a streaming query. Enter the command in your next Jupyter cell.", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 13, "cell_type": "code", "source": "// Stream from Kafka\nval kafkaStreamDF = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafkaBrokers).option(\"subscribe\", kafkaTopic).option(\"startingOffsets\", \"earliest\").load()\n\n// Select data from the stream and write to file\nkafkaStreamDF.select(from_json(col(\"value\").cast(\"string\"), schema) as \"trip\").writeStream.format(\"parquet\").option(\"path\",\"/nyctaxiraw/streamingtripdata\").option(\"checkpointLocation\", \"/streamcheckpoint\").start.awaitTermination(30000)\nprintln(\"Wrote data to file\")", "outputs": [{"output_type": "stream", "name": "stderr", "text": "java.lang.IllegalStateException: Cannot start query with id 0c1f5828-afbe-4204-9ccc-df128b9afc54 as another query with same id is already active. Perhaps you are attempting to restart a query from checkpoint that is already active.\n  at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:300)\n  at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:282)\n  ... 58 elided\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 1262.576904296875, "end_time": 1595702120702.589}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "### Run the following cell to verify that the files were written by the streaming query.", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 14, "cell_type": "code", "source": "%%bash\nhdfs dfs -ls /nyctaxiraw/streamingtripdata", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Found 1 items\ndrwxr-xr-x   - livy supergroup          0 2020-07-25 18:35 /nyctaxiraw/streamingtripdata/_spark_metadata\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 2352.0849609375, "end_time": 1595702148270.149}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}